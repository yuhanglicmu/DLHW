{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxiZ42B4SwQ-"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tests_hw4 import test_prediction, test_generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5znxQhLSwRC"
      },
      "outputs": [],
      "source": [
        "# load all that we need\n",
        "\n",
        "dataset = np.load('../dataset/wiki.train.npy', allow_pickle=True)\n",
        "devset = np.load('../dataset/wiki.valid.npy', allow_pickle=True)\n",
        "fixtures_pred = np.load('../fixtures/prediction.npz')  # dev\n",
        "fixtures_gen = np.load('../fixtures/generation.npy')  # dev\n",
        "fixtures_pred_test = np.load('../fixtures/prediction_test.npz')  # test\n",
        "fixtures_gen_test = np.load('../fixtures/generation_test.npy')  # test\n",
        "vocab = np.load('../dataset/vocab.npy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZNrJ8XvSwRF"
      },
      "outputs": [],
      "source": [
        "# data loader\n",
        "\n",
        "class DataLoaderForLanguageModeling(DataLoader):\n",
        "    \"\"\"\n",
        "        TODO: Define data loader logic here\n",
        "    \"\"\"\n",
        "    def __init__(self, dataset, batch_size, shuffle=True):\n",
        "        self.dataset = dataset# TODO\n",
        "        self.batch_size = batch_size# TODO\n",
        "        self.shuffle = shuffle# TODO \n",
        "        self.sequence_length = 3\n",
        "        \n",
        "       # raise NotImplemented\n",
        "\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"\n",
        "            You may implement some of the techniques in https://arxiv.org/pdf/1708.02182.pdf\n",
        "            example: Variable length backpropagation sequences (Section 4.1)\n",
        "        \"\"\"\n",
        "        # 1. Randomly shuffle all the articles from the WikiText-2 dataset.\n",
        "        # 2. Concatenate all text in one long string.\n",
        "        # 3. Group the sequences into batches.\n",
        "        # 4. Run a loop that returns a tuple of (input, label) on every iteration with yield.\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.dataset)\n",
        "        # Concatenate articles and drop extra words\n",
        "        data_concat = np.concatenate(self.dataset)\n",
        "        numBatches = (len(data_concat)-1) // self.batch_size\n",
        "        x = torch.from_numpy(data_concat[:numBatches*self.batch_size].reshape(self.batch_size, -1)).type(torch.LongTensor)\n",
        "        y = torch.from_numpy(data_concat[1:numBatches*self.batch_size+1].reshape(self.batch_size, -1)).type(torch.LongTensor)\n",
        "        currIdx = 0\n",
        "        while currIdx < numBatches:\n",
        "            # BPTT variable length setting from paper\n",
        "            p = np.random.random_sample()\n",
        "            if p < 0.95:\n",
        "                seqLen = round(np.random.normal(70, 5))\n",
        "            else:\n",
        "                seqLen = round(np.random.normal(35, 5))\n",
        "            currX = x[:, currIdx:currIdx+seqLen]\n",
        "            currY = y[:, currIdx:currIdx+seqLen]\n",
        "            currIdx = currIdx + seqLen\n",
        "            yield currX, currY # (B, SeqLen)\n",
        "        \n",
        "        # yield (inputs, targets)\n",
        "\n",
        "        \n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zt-7YsTYSwRI"
      },
      "outputs": [],
      "source": [
        "# model\n",
        "\n",
        "class Model(nn.Module):\n",
        "    \"\"\"\n",
        "        TODO: Define your model here\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size:int, embedding_dim:int, hidden_size:int):\n",
        "        super(Model, self).__init__()\n",
        "        #model\n",
        "        self.embedding = nn.Embedding(vocab_size, 400)\n",
        "        self.rnns = nn.LSTM(input_size=400, hidden_size=1150, num_layers=3, batch_first=True)\n",
        "        self.linear = nn.Linear(1150, vocab_size)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Feel free to add extra arguments to forward (like an argument to pass in the hiddens)\n",
        "        embedded = self.embedding(x)\n",
        "        lstm_out, hidden = self.rnns(embedded)\n",
        "        out = self.linear(lstm_out)\n",
        "        return out, hidden\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIvZOIfjSwRK"
      },
      "outputs": [],
      "source": [
        "# model trainer\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, loader, max_epochs=1, run_id='exp'):\n",
        "        \"\"\"\n",
        "            Use this class to train your model\n",
        "        \"\"\"\n",
        "        # feel free to add any other parameters here\n",
        "        self.model = model\n",
        "        self.loader = loader\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.predictions = []\n",
        "        self.predictions_test = []\n",
        "        self.generated_logits = []\n",
        "        self.generated = []\n",
        "        self.generated_logits_test = []\n",
        "        self.generated_test = []\n",
        "        self.epochs = 0\n",
        "        self.max_epochs = max_epochs\n",
        "        self.run_id = run_id\n",
        "        \n",
        "        # TODO: Define your optimizer and criterion here\n",
        "        # feel free to define a learning rate scheduler as well if you want\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), 1e-3, weight_decay=1e-6)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def train(self):\n",
        "        self.model.train() # set to training mode\n",
        "        epoch_loss = 0\n",
        "        num_batches = 0\n",
        "        for batch_num, (inputs, targets) in enumerate(self.loader):\n",
        "            epoch_loss += self.train_batch(inputs, targets)\n",
        "        epoch_loss = epoch_loss / (batch_num + 1)\n",
        "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, epoch_loss))\n",
        "        self.train_losses.append(epoch_loss)\n",
        "\n",
        "    def train_batch(self, inputs, targets):\n",
        "        \"\"\" \n",
        "            TODO: Define code for training a single batch of inputs\n",
        "            \n",
        "            :return \n",
        "                    (float) loss value\n",
        "        \"\"\"\n",
        "        inputs = inputs.cuda()\n",
        "        targets = targets.cuda()\n",
        "        outputs, _ = self.model(inputs)\n",
        "        loss = self.criterion(outputs.view(-1, outputs.size(2)), targets.view(-1))\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss\n",
        "\n",
        "    \n",
        "    def test(self):\n",
        "        # don't change these\n",
        "        self.model.eval() # set to eval mode\n",
        "        predictions = TestLanguageModel.predict(fixtures_pred['inp'], self.model) # get predictions\n",
        "        self.predictions.append(predictions)\n",
        "        generated_logits = TestLanguageModel.generate(fixtures_gen, 10, self.model) # generated predictions for 10 words\n",
        "        generated_logits_test = TestLanguageModel.generate(fixtures_gen_test, 10, self.model)\n",
        "        nll = test_prediction(predictions, fixtures_pred['out'])\n",
        "        generated = test_generation(fixtures_gen, generated_logits, vocab)\n",
        "        generated_test = test_generation(fixtures_gen_test, generated_logits_test, vocab)\n",
        "        self.val_losses.append(nll)\n",
        "        \n",
        "        self.generated.append(generated)\n",
        "        self.generated_test.append(generated_test)\n",
        "        self.generated_logits.append(generated_logits)\n",
        "        self.generated_logits_test.append(generated_logits_test)\n",
        "        \n",
        "        # generate predictions for test data\n",
        "        predictions_test = TestLanguageModel.predict(fixtures_pred_test['inp'], self.model) # get predictions\n",
        "        self.predictions_test.append(predictions_test)\n",
        "            \n",
        "        print('[VAL]  Epoch [%d/%d]   Loss: %.4f'\n",
        "                      % (self.epochs + 1, self.max_epochs, nll))\n",
        "        self.epochs += 1\n",
        "\n",
        "        return nll\n",
        "\n",
        "    def save(self):\n",
        "        # don't change these\n",
        "        model_path = os.path.join('experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
        "        torch.save({'state_dict': self.model.state_dict()},\n",
        "            model_path)\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-{}.npy'.format(self.epochs)), self.predictions[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'predictions-test-{}.npy'.format(self.epochs)), self.predictions_test[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-{}.npy'.format(self.epochs)), self.generated_logits[-1])\n",
        "        np.save(os.path.join('experiments', self.run_id, 'generated_logits-test-{}.npy'.format(self.epochs)), self.generated_logits_test[-1])\n",
        "        with open(os.path.join('experiments', self.run_id, 'generated-{}.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated[-1])\n",
        "        with open(os.path.join('experiments', self.run_id, 'generated-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated_test[-1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xPI7_kZRSwRN"
      },
      "outputs": [],
      "source": [
        "class TestLanguageModel:\n",
        "    def predict(inp, model):\n",
        "        \"\"\"\n",
        "            TODO: write prediction code here\n",
        "            \n",
        "            :param inp:\n",
        "            :return: a np.ndarray of logits\n",
        "        \"\"\"\n",
        "        inp = torch.cuda.LongTensor(inp)\n",
        "        out, _= model(inp)\n",
        "        pred = out[:, -1]\n",
        "        return pred.cpu().detach().numpy()\n",
        "\n",
        "        \n",
        "    def generate(inp, forward, model):\n",
        "        \"\"\"\n",
        "            TODO: write generation code here\n",
        "\n",
        "            Generate a sequence of words given a starting sequence.\n",
        "            :param inp: Initial sequence of words (batch size, length)\n",
        "            :param forward: number of additional words to generate\n",
        "            :return: generated words (batch size, forward)\n",
        "        \"\"\"        \n",
        "        model.eval()\n",
        "        with torch.set_grad_enabled(False):\n",
        "            res = []\n",
        "            inp = torch.cuda.LongTensor(inp)\n",
        "            out, hidden = model(inp)\n",
        "            currWord = torch.argmax(out, dim=2)[:,-1].unsqueeze(1)\n",
        "            res.append(currWord)\n",
        "            if forward > 1:\n",
        "                for i in range(forward - 1):\n",
        "                    out, hidden = model(currWord, hidden)\n",
        "                    currWord = torch.argmax(out, dim=2)[:,-1].unsqueeze(1)\n",
        "                    res.append(currWord)\n",
        "        return torch.cat(res, dim=1).cpu().detach().numpy()\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiUrjbEjSwRQ"
      },
      "outputs": [],
      "source": [
        "# TODO: define other hyperparameters here\n",
        "\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 60\n",
        "EMB_DIM = None\n",
        "HIDDEN_SIZE = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HCVG5YISwRW"
      },
      "outputs": [],
      "source": [
        "run_id = str(int(time.time()))\n",
        "if not os.path.exists('./experiments'):\n",
        "    os.mkdir('./experiments')\n",
        "os.mkdir('./experiments/%s' % run_id)\n",
        "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbHH6zXTSwRa"
      },
      "outputs": [],
      "source": [
        "model = Model(len(vocab), embedding_dim=EMB_DIM, hidden_size=HIDDEN_SIZE)\n",
        "\n",
        "loader = DataLoaderForLanguageModeling(\n",
        "    dataset=dataset, \n",
        "    batch_size=BATCH_SIZE, \n",
        "    shuffle=True\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=model, \n",
        "    loader=loader, \n",
        "    max_epochs=NUM_EPOCHS, \n",
        "    run_id=run_id\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7D8wTJkBSwRc"
      },
      "outputs": [],
      "source": [
        "best_nll = 1e30 \n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    trainer.train()\n",
        "    nll = trainer.test()\n",
        "    if nll < best_nll:\n",
        "        best_nll = nll\n",
        "        print(\"Saving model, predictions and generated output for epoch \"+str(epoch)+\" with NLL: \"+ str(best_nll))\n",
        "        trainer.save()\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z2FmDqBCSwRf"
      },
      "outputs": [],
      "source": [
        "# Don't change these\n",
        "# plot training curves\n",
        "plt.figure()\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation losses')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('NLL')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipdbmqaGSwRh"
      },
      "outputs": [],
      "source": [
        "# see generated output\n",
        "print (trainer.generated[-1]) # get last generated output"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 ('SPML')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "0ae150afb3eeae3f04d9935b4ff364ee8644ee0c11b429f8500866486e26175b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
